{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of K-nearest neighbors (KNN) algorithm for human face recognition\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Authors: Maria Chiritescu, Georgi Georgiev, Gabriela Marinova and Deniz Akinbosoye\n",
    "\n",
    "## July 2021\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "(...)\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Introduction \n",
    "2. Our Dataset\n",
    "3. Methods\n",
    "\n",
    "    3.1 Implementing Principle component analysis (PCA) algorithm\n",
    "    \n",
    "    3.2 Implementing K-nearest neighbors (KNN) algorithm\n",
    "    \n",
    "    \n",
    "5. Further applications\n",
    "6. Results\n",
    "7. Discussion\n",
    "8. References\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "## Our Dataset\n",
    "The Extended Yale Face Database B is an online-available folder, that can be easily downloaded on each computer and which represents the raw material that the program uses. The folder contains 39 subfolders, each subfolder containing 64 pictures of one person and only one. The pictures from each subfolder are of “.pgm” format and differ from one another in illumination conditions. All pictures are grayscale and have the dimension of 168×192 pixels. \n",
    "If given a new picture to analyze, the program is able to tell whether or not the person in that picture is in the dataset and if the answer is affirmative, to return the name of the subfolder corresponding to that certain person. It is worth mentioning that the ideal case would be to return the name of the person, however these names were not available, so returning the name of the subfolder dedicated to them appears to be the most appropriate result of the program.\n",
    "Considering the fact that there is a limited number of pictures for each person, no extra picture can be found in order to test the program’s ability to recognize the individual in it. In this respect, the dataset has to be modified the following way. A number of 13 pictures were extracted from each subfolder and transfered into the testing set. In other words, the testing set contains pictures of people from the dataset, which the program has to recognize. The remaining 51 pictures in each folder make up the training set, so it contains to images with which the program has to compare each picture from the testing set:\n",
    "It is worth mentioning that the original dataset contained a number of pictures with the extension „.bad”. The light illumination in this pictures reveals very few and vague characteristics of the person inside of it, so that they would not deliver relevant information to the program when analyzed. On this line, because these „.bad” they look very much alike with one another, they were represent a disadvantage to the program ans, as such, were left out of both the testing and the training set.\n",
    "\n",
    "\n",
    "##  Implementing principle component analysis (PCA) algorithm\n",
    "Now we move to our PCA class. To prepare the images for the PCA we need to center them by calculating the mean image which is done by finding the mean value of each pixel of the images in the dataset. The mean image is then subtracted from every image in the Dataset and saved as a normalized matrix. The PCA can be done in one of two was using Eigen Value Decomposition (EVD) Singular Value Decomposition (SVD). The number of components is calculated by means of a cumulative sum of the explained variance (for this the eigenvalues need to be calculated). EVD calculates the eigenvalues and eigenvectors of the covariance matrix of our image matrix, then uses the calculated eigenvectors to transform the original matrix and thus reduce its dimensions. As the dimensionality reduction is performed only on the pixels the transformation matrix the eigenvector matrix should have a shape (number of original pixels x number of components) in order for the matrix multiplication to make sense. Since this requires a covariance matrix of a shape (number of original pixels x number of original pixels) the computational time for this method has made it unpractical. The less time-consuming method for dimensionality reduction is SVD. It calculates three matrices U, S and Vt if G is our image matrix then: U has the eigenvectors of G.Gt as columns,  S has square roots of the eigenvalues of G.Gt and Gt.G in its diagonal and  V has the eigenvectors of Gt.G as columns.\n",
    "\n",
    "\n",
    "## Implementing K-nearest neighbors (KNN) algorithm\n",
    "\n",
    "K- nearest neighbors is one of the simplest Machine Learning algorithms which assumes the similarity between new data and already available data. Moreover, it assigns the new category to its closest neighbors from the established categories as well as to their labels (Peterson, 2009). \n",
    "Based on the given dataset, this method classifies the test - dataset by comparing the distance between the pixels of every single image in the testing data and the pixels of all images in the training data. For distance measurement a so called Euclidean distance has been calculated, which is done by using the following formula, where the variable \"xi\" represents the arrays of the pictures from training set after tranformation with PCA method and \"yi\" variable shows the PCA transformed arrays of every single image from testing set (Aufmann et al. 2007). \n",
    "\n",
    "$$ d_{x,y} = \\sqrt{(\\sum_{i=1}^{n}({x}i-{y}i)^2)}$$\n",
    "\n",
    "Since the distances has been determined, they are sorted by increasing values, where the smalles are used for label allocation. The number of distances corresponds to the number of k, therefore, by altering the k value, different amount of distances between images has been observed. The aim of these changes is to find the most reliable value of k, so that as many pictures as possible are assigned to their correct labels. In this case the labels are the names of the folders, where each folder contains the images of one different person.\n",
    "\n",
    "## References\n",
    "\n",
    "Peterson, LE. (2009). K-nearest neighbor. *Scholarpedia* 1883, 4.\n",
    "\n",
    "Aufmann, Richard N., Barker, Vernon C., Nation, Richard D. (2007). College Trigonometry, Sixth Edition. Cengage Learning, p. 17\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
